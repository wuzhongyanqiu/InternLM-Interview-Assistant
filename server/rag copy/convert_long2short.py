import hanlp

# 加载 HanLP 中文 NER 模型
ner_model = hanlp.load('ner/msra_bert_base')

# 示例中文文本
text = """
- 交叉熵损失（对数损失）：对于二分类问题，通常使用二元交叉熵，它衡量的是模型预测概率与实际标签之间的差异；对于多分类问题，使用多类交叉熵或称为softmax损失，优点是能有效区分类别，是分类问题的标准损失函数，缺点是对于不平衡数据集，可能会导致对多数类的过拟合。
- Hinge Loss：主要用于支持向量机，它鼓励模型尽可能地将数据点正确分类且远离决策边界。优点是可以直接最大化间隔，使决策边界更加鲁棒；缺点是计算成本较高，且对误分类点的惩罚不是连续的。
- 感知器损失：主要用于简单的二分类问题，它测量的是模型预测值和实际标签之间的差异。优点是简单直观，易于实现；缺点是对于多分类问题不适用，且对于非线性可分数据效果不佳。
- 平滑Hinge Loss：是Hinge Loss的改进版本，使得损失函数在决策边界附近连续可导，从而更容易优化。

选择合适的损失函数取决于具体任务的性质、数据分布以及模型的优化目标。在实践中，通常会尝试多种损失函数，并结合验证集的表现来选择最佳的损失函数。
"""

# 进行处理
result = ner_model(text)
print(result)